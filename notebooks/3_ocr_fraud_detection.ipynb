{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da353ba",
   "metadata": {},
   "source": [
    "# T√¢che 3 : D√©tection de Fraude OCR dans les Documents d'Identit√©\n",
    "\n",
    "Ce notebook a pour objectif de r√©soudre la troisi√®me t√¢che du challenge ANIP : d√©tecter les fraudes dans les documents d'identit√© en utilisant des techniques d'OCR et de vision par ordinateur.\n",
    "\n",
    "## Objectif\n",
    "\n",
    "D√©velopper une IA capable de distinguer automatiquement les documents d'identit√© authentiques des documents falsifi√©s, en analysant :\n",
    "- Les incoh√©rences visuelles\n",
    "- Les anomalies textuelles via OCR\n",
    "- Les patterns de falsification\n",
    "\n",
    "## Types de Documents Analys√©s\n",
    "\n",
    "Notre dataset contient des documents de diff√©rents pays/types :\n",
    "- **Arizona DL** : Permis de conduire d'Arizona\n",
    "- **ESP** : Documents espagnols  \n",
    "- **EST** : Documents estoniens\n",
    "- **RUS** : Documents russes\n",
    "\n",
    "## Strat√©gie Adopt√©e\n",
    "\n",
    "Notre approche sera m√©thodique et multi-modale :\n",
    "\n",
    "1. **Analyse Exploratoire des Donn√©es** : Comprendre les types de fraudes et leur distribution\n",
    "2. **Pr√©paration des Donn√©es** : Organisation par type de document et de fraude\n",
    "3. **Extraction de Features** : OCR + Features visuelles + Analyse de qualit√© d'image\n",
    "4. **Mod√©lisation Multi-Classe** : Classification binaire et d√©tection du type de fraude\n",
    "5. **√âvaluation Robuste** : M√©triques adapt√©es √† la s√©curit√© (pr√©cision sur les fraudes)\n",
    "6. **Pipeline de Production** : Syst√®me de d√©tection en temps r√©el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des biblioth√®ques fondamentales\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Biblioth√®ques pour OCR et traitement d'image\n",
    "try:\n",
    "    import pytesseract\n",
    "    print(\"‚úÖ Tesseract OCR disponible\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Tesseract OCR non install√© - installation n√©cessaire\")\n",
    "\n",
    "# Biblioth√®ques ML/DL\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Configuration du device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device utilis√©: {device}\")\n",
    "print(\"‚úÖ Biblioth√®ques de base import√©es.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins de donn√©es\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Chemins vers les donn√©es de la t√¢che 3\n",
    "DATA_ROOT = Path('/workspaces/anip-facial-ocr-challenge/dataset_tache_3/dataset_tache_3')\n",
    "TRAIN_DIR = DATA_ROOT / 'train'\n",
    "SUBMISSIONS_DIR = Path('/workspaces/anip-facial-ocr-challenge/submissions')\n",
    "\n",
    "print(\"üéØ Configuration des chemins:\")\n",
    "print(f\"   üìÅ Dataset root: {DATA_ROOT}\")\n",
    "print(f\"   üöÇ Train directory: {TRAIN_DIR}\")\n",
    "print(f\"   üì§ Submissions: {SUBMISSIONS_DIR}\")\n",
    "\n",
    "# V√©rification de l'existence des donn√©es\n",
    "train_exists = TRAIN_DIR.exists()\n",
    "print(f\"\\nüîç V√©rification:\")\n",
    "print(f\"   Train folder: {'‚úÖ' if train_exists else '‚ùå'} {TRAIN_DIR}\")\n",
    "\n",
    "if train_exists:\n",
    "    # Explorer la structure des donn√©es\n",
    "    document_types = [d for d in TRAIN_DIR.iterdir() if d.is_dir()]\n",
    "    print(f\"\\nüìä Types de documents trouv√©s: {len(document_types)}\")\n",
    "    \n",
    "    for doc_type in document_types:\n",
    "        print(f\"\\nüìã {doc_type.name.upper()}:\")\n",
    "        subdirs = [d for d in doc_type.iterdir() if d.is_dir()]\n",
    "        \n",
    "        for subdir in subdirs:\n",
    "            # Compter les images dans chaque sous-dossier\n",
    "            image_files = []\n",
    "            for ext in ['*.jpg', '*.JPG', '*.jpeg', '*.JPEG', '*.png', '*.PNG']:\n",
    "                image_files.extend(list(subdir.glob(ext)))\n",
    "            \n",
    "            print(f\"   üìÅ {subdir.name}: {len(image_files)} images\")\n",
    "            \n",
    "            # Afficher quelques exemples de noms de fichiers\n",
    "            if len(image_files) > 0:\n",
    "                print(f\"      üîé Exemples: {', '.join([f.name for f in image_files[:3]])}\")\n",
    "                if len(image_files) > 3:\n",
    "                    print(f\"         ... et {len(image_files) - 3} autres\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Les donn√©es de la t√¢che 3 ne sont pas disponibles!\")\n",
    "    print(\"   üí° Assure-toi que le dataset est bien plac√© dans le bon r√©pertoire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15020b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un DataFrame complet avec toutes les images et leurs labels\n",
    "def create_fraud_dataset():\n",
    "    \"\"\"\n",
    "    Cr√©e un DataFrame avec toutes les images, leurs chemins et labels\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    if not TRAIN_DIR.exists():\n",
    "        print(\"‚ùå Donn√©es non disponibles\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    document_types = [d for d in TRAIN_DIR.iterdir() if d.is_dir()]\n",
    "    \n",
    "    for doc_type in document_types:\n",
    "        doc_name = doc_type.name\n",
    "        subdirs = [d for d in doc_type.iterdir() if d.is_dir()]\n",
    "        \n",
    "        for subdir in subdirs:\n",
    "            category = subdir.name\n",
    "            \n",
    "            # D√©terminer le label de fraude\n",
    "            if category == 'normal':\n",
    "                fraud_label = 'authentic'\n",
    "                fraud_type = 'none'\n",
    "            elif category == 'gt':\n",
    "                fraud_label = 'ground_truth'\n",
    "                fraud_type = 'reference'\n",
    "            elif category.startswith('forgery_'):\n",
    "                fraud_label = 'fraud'\n",
    "                fraud_type = category\n",
    "            else:\n",
    "                fraud_label = 'unknown'\n",
    "                fraud_type = category\n",
    "            \n",
    "            # R√©cup√©rer toutes les images\n",
    "            image_files = []\n",
    "            for ext in ['*.jpg', '*.JPG', '*.jpeg', '*.JPEG', '*.png', '*.PNG']:\n",
    "                image_files.extend(list(subdir.glob(ext)))\n",
    "            \n",
    "            # Ajouter chaque image au dataset\n",
    "            for img_path in image_files:\n",
    "                data_list.append({\n",
    "                    'file_path': str(img_path),\n",
    "                    'filename': img_path.name,\n",
    "                    'document_type': doc_name,\n",
    "                    'category': category,\n",
    "                    'fraud_label': fraud_label,\n",
    "                    'fraud_type': fraud_type,\n",
    "                    'file_size': img_path.stat().st_size if img_path.exists() else 0\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "# Cr√©er le dataset principal\n",
    "print(\"üîç CR√âATION DU DATASET DE FRAUDE DOCUMENTAIRE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_fraud = create_fraud_dataset()\n",
    "\n",
    "if len(df_fraud) > 0:\n",
    "    print(f\"‚úÖ Dataset cr√©√© avec succ√®s!\")\n",
    "    print(f\"   Total d'images: {len(df_fraud):,}\")\n",
    "    print(f\"   Types de documents: {df_fraud['document_type'].nunique()}\")\n",
    "    print(f\"   Cat√©gories: {df_fraud['category'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nüìä Aper√ßu du dataset:\")\n",
    "    print(df_fraud.head(10))\n",
    "    \n",
    "    print(f\"\\nüìã Informations sur le dataset:\")\n",
    "    print(df_fraud.info())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Impossible de cr√©er le dataset - donn√©es non disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse Exploratoire des Donn√©es de Fraude\n",
    "\n",
    "Maintenant que nous avons explor√© la structure, analysons en d√©tail les diff√©rents types de fraudes et leurs caract√©ristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse Exploratoire des Donn√©es de Fraude\n",
    "\n",
    "Maintenant que nous avons explor√© la structure, analysons en d√©tail les diff√©rents types de fraudes et leurs caract√©ristiques."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
